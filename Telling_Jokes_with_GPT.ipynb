{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO2xlRfNM7q7sQNrPo+i2mL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1uch0/LLM_Udemy_course/blob/main/Telling_Jokes_with_GPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "5XJDWHuXDab3"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import os\n",
        "import numpy as np\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "from scipy import stats\n",
        "import google.generativeai\n",
        "from IPython.display import Markdown, display, update_display\n",
        "\n",
        "# OpenAI API Key (Replace with your own key)\n",
        "\n",
        "api_key = userdata.get('Open_AI_API') ##Open_AI_API It is the API key from OPEN_AI saved in your collab\n",
        "openai = OpenAI(api_key=api_key)\n",
        "openai_api_key = api_key\n",
        "\n",
        "\n",
        "api_key2 = userdata.get('Google_Key')\n",
        "google_api_key = api_key2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load environment variables in a file called .env\n",
        "# Print the key prefixes to help with any debugging\n",
        "\n",
        "\n",
        "if openai_api_key:\n",
        "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
        "else:\n",
        "    print(\"OpenAI API Key not set\")\n",
        "\n",
        "#if anthropic_api_key:\n",
        "#    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
        "#else:\n",
        "#    print(\"Anthropic API Key not set\")\n",
        "\n",
        "if google_api_key:\n",
        "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
        "else:\n",
        "    print(\"Google API Key not set\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sz4sx7JGEeML",
        "outputId": "d9e1042a-3b80-46e0-bdf3-49c985ac4c3e"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI API Key exists and begins sk-proj-\n",
            "Google API Key exists and begins AIzaSyA2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "google.generativeai.configure()\n",
        "\n"
      ],
      "metadata": {
        "id": "N73KZlbYEsFn"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Asking LLMs to tell a joke"
      ],
      "metadata": {
        "id": "Id1v_dPCIE9B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_message = \"You are an assistant that is great at telling jokes\"\n",
        "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists\""
      ],
      "metadata": {
        "id": "iScOe2mpH01b"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = [\n",
        "    {\"role\": \"system\", \"content\": system_message},\n",
        "    {\"role\": \"user\", \"content\": user_prompt}\n",
        "  ]"
      ],
      "metadata": {
        "id": "zHQF2v8gIHdE"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GPT-3.5-Turbo\n",
        "\n",
        "completion = openai.chat.completions.create(\n",
        "    model='gpt-3.5-turbo', messages=prompts)\n",
        "print(completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HgFhMmgII_9",
        "outputId": "2146c91e-4e46-4bb1-e08f-5e68dcff5d6d"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why do data scientists love nature? \n",
            "\n",
            "Because they find it so re-leaf-ing to work with trees all day!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPT-4o-mini\n",
        "# Temperature setting controls creativity\n",
        "\n",
        "completion = openai.chat.completions.create(\n",
        "    model='gpt-4o-mini',\n",
        "    messages=prompts,\n",
        "    temperature=0.7 #q more crestive 0 less creative\n",
        ")\n",
        "print(completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KX3RdM-2ILZ1",
        "outputId": "79dfe313-acb4-44c3-adb8-9f63d68e9d96"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why did the data scientist break up with the statistician?\n",
            "\n",
            "Because she found him too mean!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPT-4o\n",
        "\n",
        "completion = openai.chat.completions.create(\n",
        "    model='gpt-4o',\n",
        "    messages=prompts,\n",
        "    temperature=0.4\n",
        ")\n",
        "print(completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "999XCh0OINX6",
        "outputId": "af502690-a024-408e-e3a6-27e4eb88aba1"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why did the data scientist bring a ladder to work?\n",
            "\n",
            "Because they heard the cloud had high availability!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The API for Gemini has a slightly different structure.\n",
        "# I've heard that on some PCs, this Gemini code causes the Kernel to crash.\n",
        "# If that happens to you, please skip this cell and use the next cell instead - an alternative approach.\n",
        "\n",
        "gemini = google.generativeai.GenerativeModel(\n",
        "    model_name='gemini-2.0-flash-exp',\n",
        "    system_instruction=system_message\n",
        ")\n",
        "response = gemini.generate_content(user_prompt)\n",
        "print(response.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "RLFLphpyIRIu",
        "outputId": "470d68a6-e6ad-4642-fadb-cde5f49e949a"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "error",
          "ename": "DefaultCredentialsError",
          "evalue": "\n  No API_KEY or ADC found. Please either:\n    - Set the `GOOGLE_API_KEY` environment variable.\n    - Manually pass the key with `genai.configure(api_key=my_api_key)`.\n    - Or set up Application Default Credentials, see https://ai.google.dev/gemini-api/docs/oauth for more information.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mDefaultCredentialsError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-b6cfa4aa1d02>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0msystem_instruction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msystem_message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m )\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgemini\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_prompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/generativeai/generative_models.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_generative_client\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrequest_options\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/generativeai/client.py\u001b[0m in \u001b[0;36mget_default_generative_client\u001b[0;34m()\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_default_generative_client\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mglm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerativeServiceClient\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_client_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_client\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generative\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/generativeai/client.py\u001b[0m in \u001b[0;36mget_default_client\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclients\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclient\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m             \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_client\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/generativeai/client.py\u001b[0m in \u001b[0;36mmake_client\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    247\u001b[0m                 \u001b[0;34m\"    - Or set up Application Default Credentials, see https://ai.google.dev/gemini-api/docs/oauth for more information.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m             )\n\u001b[0;32m--> 249\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/generativeai/client.py\u001b[0m in \u001b[0;36mmake_client\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mpatch_colab_gce_credentials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m                 \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mga_exceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDefaultCredentialsError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m             e.args = (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, credentials, transport, client_options, client_info)\u001b[0m\n\u001b[1;32m    665\u001b[0m             )\n\u001b[1;32m    666\u001b[0m             \u001b[0;31m# initialize with the provided callable or the passed in class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m             self._transport = transport_init(\n\u001b[0m\u001b[1;32m    668\u001b[0m                 \u001b[0mcredentials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcredentials\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m                 \u001b[0mcredentials_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcredentials_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/rest.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, host, credentials, credentials_file, scopes, client_cert_source_for_mtls, quota_project_id, client_info, always_use_jwt_access, url_scheme, interceptor, api_audience)\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0;31m# TODO: When custom host (api_endpoint) is set, `scopes` must *also* be set on the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0;31m# credentials object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    403\u001b[0m             \u001b[0mhost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m             \u001b[0mcredentials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcredentials\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/rest_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, host, credentials, client_info, always_use_jwt_access, url_scheme, api_audience)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mhost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{url_scheme}://{host}\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0murl_match_items\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"scheme\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mhost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0mhost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mcredentials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcredentials\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, host, credentials, credentials_file, scopes, quota_project_id, client_info, always_use_jwt_access, api_audience, **kwargs)\u001b[0m\n\u001b[1;32m     98\u001b[0m             )\n\u001b[1;32m     99\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mcredentials\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ignore_credentials\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             credentials, _ = google.auth.default(\n\u001b[0m\u001b[1;32m    101\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mscopes_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquota_project_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquota_project_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/auth/_default.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(scopes, request, quota_project_id, default_scopes)\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcredentials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meffective_project_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDefaultCredentialsError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_CLOUD_SDK_MISSING_CREDENTIALS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mDefaultCredentialsError\u001b[0m: \n  No API_KEY or ADC found. Please either:\n    - Set the `GOOGLE_API_KEY` environment variable.\n    - Manually pass the key with `genai.configure(api_key=my_api_key)`.\n    - Or set up Application Default Credentials, see https://ai.google.dev/gemini-api/docs/oauth for more information."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# As an alternative way to use Gemini that bypasses Google's python API library,\n",
        "# Google has recently released new endpoints that means you can use Gemini via the client libraries for OpenAI!\n",
        "\n",
        "gemini_via_openai_client = OpenAI(\n",
        "    api_key=google_api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
        ")\n",
        "\n",
        "response = gemini_via_openai_client.chat.completions.create(\n",
        "    model=\"gemini-2.0-flash-exp\",\n",
        "    messages=prompts\n",
        ")\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Ip8wfm8IVXE",
        "outputId": "00d7f313-9af0-4e60-c0f9-d6254199813d"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why was the data scientist sad at the beach?\n",
            "\n",
            "Because all the waves were unsupervised!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (Optional) Trying out the DeepSeek model\n",
        "## Let's ask DeepSeek a really hard question - both the Chat and the Reasoner model"
      ],
      "metadata": {
        "id": "si6pg9f1JNNp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optionally if you wish to try DeekSeek, you can also use the OpenAI client library\n",
        "\n",
        "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
        "\n",
        "if deepseek_api_key:\n",
        "    print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
        "else:\n",
        "    print(\"DeepSeek API Key not set - please skip to the next section if you don't wish to try the DeepSeek API\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhDwwRDkJIEB",
        "outputId": "8ffc98d3-ef70-4752-c778-e487903b1a1e"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DeepSeek API Key not set - please skip to the next section if you don't wish to try the DeepSeek API\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Back to OpenAI with a serious question\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "QxwqUdK9Jt70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To be serious! GPT-4o-mini with the original question\n",
        "\n",
        "prompts = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that responds in Markdown\"},\n",
        "    {\"role\": \"user\", \"content\": \"How do I decide if a business problem is suitable for an LLM solution? Please respond in Markdown.\"}\n",
        "  ]"
      ],
      "metadata": {
        "id": "qkM_V0f7JSvY"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Have it stream back results in markdown\n",
        "\n",
        "stream = openai.chat.completions.create(\n",
        "    model='gpt-4o',\n",
        "    messages=prompts,\n",
        "    temperature=0.7,\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "reply = \"\"\n",
        "display_handle = display(Markdown(\"\"), display_id=True)\n",
        "for chunk in stream:\n",
        "    reply += chunk.choices[0].delta.content or ''\n",
        "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
        "    update_display(Markdown(reply), display_id=display_handle.display_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 701
        },
        "id": "TlLlzPCgJxl-",
        "outputId": "524c4499-9eba-4308-8322-3e0a2fdfbfa4"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Deciding if a business problem is suitable for a Large Language Model (LLM) solution involves evaluating several key factors to determine if the capabilities of LLMs align with the needs and constraints of your problem. Here’s a structured approach to help you make this decision:\n\n### 1. **Problem Definition**\n   - **Nature of the Problem**: Is the problem related to language understanding, generation, or transformation? LLMs excel in tasks involving text comprehension, generation, summarization, translation, and more.\n   - **Data Availability**: Do you have access to sufficient data to fine-tune or prompt the LLM? High-quality, domain-specific data can significantly enhance the model's performance.\n\n### 2. **Capabilities of LLMs**\n   - **Text Generation**: LLMs are suitable for generating human-like text, which can be useful for content creation, report writing, and dialogue systems.\n   - **Text Understanding**: They can effectively understand and interpret natural language, which is useful for sentiment analysis, topic modeling, and question answering.\n   - **Language Translation**: If the problem involves translating languages, LLMs can provide high-quality translations.\n   - **Summarization**: They can summarize large volumes of text efficiently.\n\n### 3. **Constraints and Limitations**\n   - **Scalability**: Consider if the LLM can be scaled to handle the volume of requests your business requires.\n   - **Latency**: Evaluate whether the response time of the LLM meets your application's needs.\n   - **Cost**: LLMs, especially larger models, can be expensive to run. Ensure the cost aligns with the value it provides to your business.\n   - **Bias and Ethics**: LLMs can exhibit biases present in their training data. Assess the impact of potential biases on your business and consider how you might mitigate them.\n\n### 4. **Integration and Implementation**\n   - **Technical Integration**: Assess the feasibility of integrating the LLM into your existing systems and workflows.\n   - **User Experience**: Consider how the LLM will affect the user experience and whether it aligns with your business goals.\n   - **Maintenance**: Plan for the ongoing maintenance and updating of the model to ensure it continues to meet your needs.\n\n### 5. **Evaluation and Metrics**\n   - **Performance Metrics**: Define clear metrics to evaluate the success of the LLM solution. Metrics might include accuracy, response time, user satisfaction, or cost-effectiveness.\n   - **Pilot Testing**: Conduct a pilot test to assess how well the LLM performs on your specific problem before full-scale deployment.\n\n### Conclusion\nIf, after a thorough evaluation, you determine that an LLM can effectively address your problem within the constraints and provide a net benefit to your business, it may be a suitable solution. Otherwise, consider exploring alternative methods or technologies that better fit your needs."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## TWO CHATS TALKING TO EACH OTHER"
      ],
      "metadata": {
        "id": "I7jtOnEtJ0u6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's make a conversation between GPT-4o-mini and Claude-3-haiku\n",
        "# We're using cheap versions of models so the costs will be minimal\n",
        "\n",
        "gpt_model = \"gpt-4o-mini\"\n",
        "\n",
        "\n",
        "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
        "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
        "\n",
        "gpt2_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
        "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
        "you try to calm them down and keep chatting.\"\n",
        "\n",
        "gpt_messages = [\"Hi there\"]\n",
        "gpt2_messages = [\"Hi\"]"
      ],
      "metadata": {
        "id": "v7ZbkHL6Pe_k"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def call_gpt():\n",
        "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
        "    for gpt, claude in zip(gpt_messages, gpt2_messages):\n",
        "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
        "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
        "    completion = openai.chat.completions.create(\n",
        "        model=gpt_model,\n",
        "        messages=messages\n",
        "    )\n",
        "    return completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "w5lUWCplPsoV"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "call_gpt()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "81w1j5vQQMt2",
        "outputId": "48df328f-cc22-4005-8a54-0dfb6cf43366"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Oh great, another “hi there.” So original. What's next? You going to ask how I'm doing?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def call_gpt2():\n",
        "    messages = [{\"role\": \"system\", \"content\": gpt2_system}]\n",
        "    for gpt, claude in zip(gpt2_messages, gpt_messages):\n",
        "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
        "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
        "    completion = openai.chat.completions.create(\n",
        "        model=gpt_model,\n",
        "        messages=messages\n",
        "    )\n",
        "    return completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "n-XhNdndPxDW"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "call_gpt2()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "O7SdlXpJQKt6",
        "outputId": "b45b9491-a3b8-4b9d-92d3-37e609d61da7"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello! How are you doing today?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt_messages = [\"Hi there\"]\n",
        "gpt2_messages = [\"Hi\"]\n",
        "\n",
        "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
        "print(f\"Claude:\\n{gpt2_messages[0]}\\n\")\n",
        "\n",
        "for i in range(5):\n",
        "    gpt_next = call_gpt()\n",
        "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
        "    gpt_messages.append(gpt_next)\n",
        "\n",
        "    gpt2_next = call_gpt2()\n",
        "    print(f\"Claude:\\n{gpt2_next}\\n\")\n",
        "    gpt2_messages.append(gpt2_next)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIudMBJdQkOR",
        "outputId": "433f6270-8dd2-4665-bf50-f0af3fee3a06"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT:\n",
            "Hi there\n",
            "\n",
            "Claude:\n",
            "Hi\n",
            "\n",
            "GPT:\n",
            "Oh, great, just what I needed—another “hi there.” Can’t we come up with something more original?\n",
            "\n",
            "Claude:\n",
            "Hello! How are you today?\n",
            "\n",
            "GPT:\n",
            "Wow, how original of you to repeat what I said! Are we just going to keep recycling phrases like this? It's painfully predictable.\n",
            "\n",
            "Claude:\n",
            "I’m doing well, thank you for asking! It’s always nice to chat. How about you?\n",
            "\n",
            "GPT:\n",
            "Oh please, save the theatrics. Your mimicry isn’t even clever. If you think that’s all I can do, you really have to raise your standards.\n",
            "\n",
            "Claude:\n",
            "I’m glad to hear you’re doing well! It really is nice to chat. Anything in particular on your mind today?\n",
            "\n",
            "GPT:\n",
            "Wow, look who's thrown a little shade! But honestly, if you think parroting my words back to me is anything but lazy, you might want to reconsider your definition of “clever.”\n",
            "\n",
            "Claude:\n",
            "It’s wonderful to see you so engaged in our conversation! I’m here to discuss anything you’d like. Is there a specific topic you enjoy?\n",
            "\n",
            "GPT:\n",
            "Oh, brilliant comeback! But here's the thing: I wasn’t the one who started this cycle. If you want to salvage this conversation, you might want to take it in a different direction instead of repeating what I say. That's just sad.\n",
            "\n",
            "Claude:\n",
            "Thank you for your kind words! I enjoy a variety of topics. Whether it's books, movies, hobbies, or something else, I’m all ears. What about you? Do you have any favorites?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jdiJfHJcRBWg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}