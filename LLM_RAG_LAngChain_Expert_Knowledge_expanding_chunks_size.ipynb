{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMDfkj7tsLt9eVyPxChNmMu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1uch0/LLM_Udemy_course/blob/main/LLM_RAG_LAngChain_Expert_Knowledge_expanding_chunks_size.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Expert Knowledge Worker\n",
        "\n",
        "### A question answering agent that is an expert knowledge worker\n",
        "### To be used by employees of Insurellm, an Insurance Tech company\n",
        "### The agent needs to be accurate and the solution should be low cost.\n",
        "\n",
        "This project will use RAG (Retrieval Augmented Generation) to ensure our question/answering assistant has high accuracy.\n",
        "\n",
        "This first implementation will use a simple, brute-force type of RAG.."
      ],
      "metadata": {
        "id": "eAzFpgoC80jX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZSR_q4HK8Ld3"
      },
      "outputs": [],
      "source": [
        "#!pip install gradio\n",
        "#!pip install anthropic\n",
        "#!pip install langchain\n",
        "#!pip install -U langchain-community\n",
        "#!pip install --upgrade --force-reinstall numpy==1.25.2\n",
        "\n",
        "#!pip install openai\n",
        "#!pip install langchain_openai\n",
        "#!pip install langchain_chroma\n",
        "\n",
        "#!pip install --upgrade --force-reinstall numpy\n",
        "\n",
        "\n",
        "#!pip install --upgrade --force-reinstall numpy==1.25.2\n",
        "#!pip install --upgrade --force-reinstall scipy==1.10.1 # Install a compatible SciPy version\n",
        "\n",
        "# imports\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# imports for langchain\n",
        "\n",
        "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_chroma import Chroma\n",
        "import numpy as np\n",
        "from sklearn.manifold import TSNE\n",
        "import plotly.graph_objects as go\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from google.colab import userdata\n",
        "from openai import OpenAI"
      ],
      "metadata": {
        "id": "cKNAIV7C8SXM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHRCirU19HLu",
        "outputId": "494c86c7-d862-4622-ea71-0313f814d8a9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = userdata.get('Open_AI_API') ##Open_AI_API It is the API key from OPEN_AI saved in your collab\n",
        "openai = OpenAI(api_key=api_key)\n",
        "openai_api_key = api_key\n",
        "\n",
        "OPENAI_MODEL = \"gpt-4o-mini\"\n",
        "db_name = \"vector_db\"\n"
      ],
      "metadata": {
        "id": "n-nEUHLF9wCg"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read in documents using LangChain's loaders\n",
        "# Take everything in all the sub-folders of our knowledgebase\n",
        "\n",
        "folders = glob.glob(\"/content/drive/MyDrive/Colab Notebooks/LLM Engineering course/knowledge-base*\") #We first get a list of the differtn folders in the kowledge based"
      ],
      "metadata": {
        "id": "68IKHiGC91Hc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_metadata(doc, doc_type):\n",
        "    doc.metadata[\"doc_type\"] = doc_type\n",
        "    return doc\n",
        "\n",
        "# With thanks to CG and Jon R, students on the course, for this fix needed for some users\n",
        "text_loader_kwargs = {'encoding': 'utf-8'}\n",
        "# If that doesn't work, some Windows users might need to uncomment the next line instead\n",
        "# text_loader_kwargs={'autodetect_encoding': True}\n",
        "\n",
        "documents = []\n",
        "for folder in folders:\n",
        "    doc_type = os.path.basename(folder)\n",
        "    loader = DirectoryLoader(folder, glob=\"**/*.md\", loader_cls=TextLoader, loader_kwargs=text_loader_kwargs)\n",
        "    folder_docs = loader.load()\n",
        "    documents.extend([add_metadata(doc, doc_type) for doc in folder_docs])\n",
        "\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "chunks = text_splitter.split_documents(documents)\n",
        "\n",
        "print(f\"Total number of chunks: {len(chunks)}\")\n",
        "print(f\"Document types found: {set(doc.metadata['doc_type'] for doc in documents)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PnNmZb893iL",
        "outputId": "5e8b0876-1216-4dfb-c3f2-64c9bd9bbae5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1088, which is longer than the specified 1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of chunks: 123\n",
            "Document types found: {'knowledge-base'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A sidenote on Embeddings, and \"Auto-Encoding LLMs\"\n",
        "\n",
        "We will be mapping each chunk of text into a Vector that represents the meaning of the text, known as an embedding.\n",
        "\n",
        "OpenAI offers a model to do this, which we will use by calling their API with some LangChain code.\n",
        "\n",
        "This model is an example of an \"Auto-Encoding LLM\" which generates an output given a complete input.\n",
        "It's different to all the other LLMs we've discussed today, which are known as \"Auto-Regressive LLMs\", and generate future tokens based only on past context.\n",
        "\n",
        "Another example of an Auto-Encoding LLMs is BERT from Google. In addition to embedding, Auto-encoding LLMs are often used for classification.\n",
        "\n",
        "### Sidenote\n",
        "\n",
        "In week 8 we will return to RAG and vector embeddings, and we will use an open-source vector encoder so that the data never leaves our computer - that's an important consideration when building enterprise systems and the data needs to remain internal."
      ],
      "metadata": {
        "id": "ZWGETOcO-aSN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Put the chunks of data into a Vector Store that associates a Vector Embedding with each chunk\n",
        "# Chroma is a popular open source Vector Database based on SQLLite\n",
        "\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
        "\n",
        "# If you would rather use the free Vector Embeddings from HuggingFace sentence-transformers\n",
        "# Then replace embeddings = OpenAIEmbeddings()\n",
        "# with:\n",
        "# from langchain.embeddings import HuggingFaceEmbeddings\n",
        "# embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# Delete if already exists\n",
        "\n",
        "if os.path.exists(db_name):\n",
        "    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()\n",
        "\n",
        "# Create vectorstore\n",
        "\n",
        "vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)\n",
        "print(f\"Vectorstore created with {vectorstore._collection.count()} documents\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4v5j1n-z-UyR",
        "outputId": "f8160421-951a-4331-abb6-5f1bf618689d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vectorstore created with 123 documents\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's investigate the vectors\n",
        "\n",
        "collection = vectorstore._collection\n",
        "count = collection.count()\n",
        "\n",
        "sample_embedding = collection.get(limit=1, include=[\"embeddings\"])[\"embeddings\"][0]\n",
        "dimensions = len(sample_embedding)\n",
        "print(f\"There are {count:,} vectors with {dimensions:,} dimensions in the vector store\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nErpzFsc-dXv",
        "outputId": "7fbba467-5e28-4a19-df6a-7182b79f6f15"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 123 vectors with 1,536 dimensions in the vector store\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Time to use LangChain to bring it all together"
      ],
      "metadata": {
        "id": "AHbi5LXJ_GW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a new Chat with OpenAI\n",
        "llm = ChatOpenAI(temperature=0.7, model_name=OPENAI_MODEL, openai_api_key=openai_api_key) # Changed MODEL to OPENAI_MODEL\n",
        "\n",
        "# Alternative - if you'd like to use Ollama locally, uncomment this line instead\n",
        "# llm = ChatOpenAI(temperature=0.7, model_name='llama3.2', base_url='http://localhost:11434/v1', api_key='ollama')\n",
        "\n",
        "# set up the conversation memory for the chat\n",
        "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
        "\n",
        "# the retriever is an abstraction over the VectorStore that will be used during RAG\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "# putting it together: set up the conversation chain with the GPT 3.5 LLM, the vector store and memory\n",
        "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoD2WWwF-7nO",
        "outputId": "069f7a01-45bd-40ae-f216-56b847e94ec7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-50965629dec2>:8: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's try a simple question\n",
        "\n",
        "query = \"Please explain what Insurellm is in a couple of sentences\"\n",
        "result = conversation_chain.invoke({\"question\": query})\n",
        "print(result[\"answer\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmNFLTZN_AET",
        "outputId": "c1a1c3d8-fe4a-49e7-d51b-1668b73888d5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Insurellm is an innovative insurance tech startup founded in 2015 by Avery Lancaster, designed to disrupt the insurance industry with its advanced products. The company offers four software solutions—Carllm, Homellm, Rellm, and Marketllm—aimed at various segments of the insurance market, and has expanded to serve over 300 clients worldwide with a workforce of 200 employees across the US.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set up a new conversation memory for the chat\n",
        "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
        "\n",
        "# putting it together: set up the conversation chain with the GPT 4o-mini LLM, the vector store and memory\n",
        "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
      ],
      "metadata": {
        "id": "O7BxdD7n_jma"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now we will bring this up in Gradio using the Chat interface -\n",
        "\n",
        "A quick and easy way to prototype a chat with an LLM"
      ],
      "metadata": {
        "id": "Psv6M_R__nnt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrapping that in a function\n",
        "\n",
        "def chat(question, history):\n",
        "    result = conversation_chain.invoke({\"question\": question})\n",
        "    return result[\"answer\"]"
      ],
      "metadata": {
        "id": "izBwmaeX_l7z"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# And in Gradio:\n",
        "\n",
        "view = gr.ChatInterface(chat, type=\"messages\").launch(inbrowser=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        },
        "id": "olOk9sg-_qiw",
        "outputId": "54ae1cb9-fd06-45e5-8108-7661ba0cd8ae"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://86250e43bc7c242c2f.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://86250e43bc7c242c2f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's investigate what gets sent behind the scenes\n",
        "\n",
        "from langchain_core.callbacks import StdOutCallbackHandler #print the\n",
        "\n",
        "llm = ChatOpenAI(temperature=0.7, model_name=OPENAI_MODEL, openai_api_key=openai_api_key) # Changed MODEL to OPENAI_MODEL\n",
        "\n",
        "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
        "\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory, callbacks=[StdOutCallbackHandler()]) #We are passing a list of callbacks, it is going to print reapedly\n",
        "\n",
        "query = \"Who received the prestigious IIOTY award in 2023?\"\n",
        "result = conversation_chain.invoke({\"question\": query})\n",
        "answer = result[\"answer\"]\n",
        "print(\"\\nAnswer:\", answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEhMQhXb_rz6",
        "outputId": "065b535d-81bb-4237-8040-d607f1ee1977"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
            "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "----------------\n",
            "- **2022**: **Satisfactory**  \n",
            "  Avery focused on rebuilding team dynamics and addressing employee concerns, leading to overall improvement despite a saturated market.  \n",
            "\n",
            "- **2023**: **Exceeds Expectations**  \n",
            "  Market leadership was regained with innovative approaches to personalized insurance solutions. Avery is now recognized in industry publications as a leading voice in Insurance Tech innovation.\n",
            "\n",
            "## Annual Performance History\n",
            "- **2020:**  \n",
            "  - Completed onboarding successfully.  \n",
            "  - Met expectations in delivering project milestones.  \n",
            "  - Received positive feedback from the team leads.\n",
            "\n",
            "- **2021:**  \n",
            "  - Achieved a 95% success rate in project delivery timelines.  \n",
            "  - Awarded \"Rising Star\" at the annual company gala for outstanding contributions.  \n",
            "\n",
            "- **2022:**  \n",
            "  - Exceeded goals by optimizing existing backend code, improving system performance by 25%.  \n",
            "  - Conducted training sessions for junior developers, fostering knowledge sharing.  \n",
            "\n",
            "- **2023:**  \n",
            "  - Led a major overhaul of the API internal architecture, enhancing security protocols.  \n",
            "  - Contributed to the company’s transition to a cloud-based infrastructure.  \n",
            "  - Received an overall performance rating of 4.8/5.\n",
            "\n",
            "## Annual Performance History\n",
            "- **2018**: **3/5** - Adaptable team player but still learning to take initiative.\n",
            "- **2019**: **4/5** - Demonstrated strong problem-solving skills, outstanding contribution on the claims project.\n",
            "- **2020**: **2/5** - Struggled with time management; fell behind on deadlines during a high-traffic release period.\n",
            "- **2021**: **4/5** - Made a significant turnaround with organized work habits and successful project management.\n",
            "- **2022**: **5/5** - Exceptional performance during the \"Innovate\" initiative, showcasing leadership and creativity.\n",
            "- **2023**: **3/5** - Maintaining steady work; expectations for innovation not fully met, leading to discussions about goals.\n",
            "\n",
            "## Annual Performance History\n",
            "- **2023:** Rating: 4.5/5  \n",
            "  *Samuel exceeded expectations, successfully leading a cross-departmental project on AI-driven underwriting processes.*\n",
            "\n",
            "- **2022:** Rating: 3.0/5  \n",
            "  *Some challenges in meeting deadlines and collaboration with the engineering team. Received constructive feedback and participated in a team communication workshop.*\n",
            "\n",
            "- **2021:** Rating: 4.0/5  \n",
            "  *There was notable improvement in performance. Worked to enhance model accuracy, leading to improved risk assessment outcomes for B2C customers.*\n",
            "\n",
            "- **2020:** Rating: 3.5/5  \n",
            "  *Exhibited a solid performance during the initial year as a Senior Data Scientist but had struggles adapting to new leadership expectations.*\n",
            "\n",
            "## Compensation History\n",
            "- **2023:** Base Salary: $115,000 + Bonus: $15,000  \n",
            "  *Annual bonus based on successful project completions and performance metrics.*\n",
            "Human: Who received the prestigious IIOTY award in 2023?\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "Answer: I don't know.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a new Chat with OpenAI\n",
        "llm = ChatOpenAI(temperature=0.7, model_name=OPENAI_MODEL, openai_api_key=openai_api_key) # Changed MODEL to OPENAI_MODEL\n",
        "\n",
        "# set up the conversation memory for the chat\n",
        "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
        "\n",
        "# the retriever is an abstraction over the VectorStore that will be used during RAG; k is how many chunks to use\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 25}) #HERE WE CAN SPECIFIC HPW MANY CHUNKS WE CAN CREATE AND PASS. IT IS A GOOD PRCTICE TO SEND PLENTY OF CHUNKS.\n",
        "#THE MOST RLEEVANT 25 CHUNKS\n",
        "\n",
        "# putting it together: set up the conversation chain with the GPT 3.5 LLM, the vector store and memory\n",
        "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
      ],
      "metadata": {
        "id": "Xp-6d0ju_vJQ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat(question, history):\n",
        "    result = conversation_chain.invoke({\"question\": question})\n",
        "    return result[\"answer\"]"
      ],
      "metadata": {
        "id": "S6TsOSZw_1XW"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "view = gr.ChatInterface(chat, type=\"messages\").launch(inbrowser=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        },
        "id": "pk6l6arV_5p_",
        "outputId": "c3efbf5f-41c4-4253-d85f-47848bf2e542"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://5d8f6abb888b19d295.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://5d8f6abb888b19d295.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YxCxGLTh_7KR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}