{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO/a6xU3rB/WVbjuqya+/pu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1uch0/LLM_Udemy_course/blob/main/LLM_RAG_LAngChain_Expert_Knowledge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Expert Knowledge Worker\n",
        "\n",
        "### A question answering agent that is an expert knowledge worker\n",
        "### To be used by employees of Insurellm, an Insurance Tech company\n",
        "### The agent needs to be accurate and the solution should be low cost.\n",
        "\n",
        "This project will use RAG (Retrieval Augmented Generation) to ensure our question/answering assistant has high accuracy."
      ],
      "metadata": {
        "id": "oU9QHgVGIPVf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "ZYsvZAQDIFqK",
        "outputId": "cf97d8c0-18e6-41b0-cf40-eacbfce310ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.25.2\n",
            "  Using cached numpy-1.25.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Using cached numpy-1.25.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-community 0.3.20 requires numpy<3,>=1.26.2, but you have numpy 1.25.2 which is incompatible.\n",
            "blosc2 3.2.0 requires numpy>=1.26, but you have numpy 1.25.2 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.25.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.25.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "9800ae00aca04e0fb446be46899d9408"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "#!pip install gradio\n",
        "#!pip install anthropic\n",
        "#!pip install langchain\n",
        "#!pip install -U langchain-community\n",
        "!pip install --upgrade --force-reinstall numpy==1.25.2\n",
        "\n",
        "#!pip install openai\n",
        "#!pip install langchain_openai\n",
        "#!pip install langchain_chroma\n",
        "\n",
        "#!pip install --upgrade --force-reinstall numpy\n",
        "\n",
        "# imports\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# imports for langchain\n",
        "\n",
        "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_chroma import Chroma\n",
        "import numpy as np\n",
        "from sklearn.manifold import TSNE\n",
        "import plotly.graph_objects as go\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from google.colab import userdata\n",
        "from openai import OpenAI"
      ],
      "metadata": {
        "id": "Ht9Z7fKFIRkU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# price is a factor for our company, so we're going to use a low cost model\n",
        "\n",
        "MODEL = \"gpt-4o-mini\"\n",
        "db_name = \"vector_db\""
      ],
      "metadata": {
        "id": "ohPxt_vSIqyY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SBYPSech-8b",
        "outputId": "9a2d0d12-25e6-49da-baa7-dac07be0f0a3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = userdata.get('Open_AI_API') ##Open_AI_API It is the API key from OPEN_AI saved in your collab\n",
        "openai = OpenAI(api_key=api_key)\n",
        "openai_api_key = api_key\n",
        "\n",
        "OPENAI_MODEL = \"gpt-4o-mini\"\n"
      ],
      "metadata": {
        "id": "NB5z2YGmjQFo"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read in documents using LangChain's loaders\n",
        "# Take everything in all the sub-folders of our knowledgebase\n",
        "\n",
        "folders = glob.glob(\"/content/drive/MyDrive/Colab Notebooks/LLM Engineering course/knowledge-base*\") #We first get a list of the differtn folders in the kowledge based\n",
        "\n",
        "# With thanks to CG and Jon R, students on the course, for this fix needed for some users\n",
        "text_loader_kwargs = {'encoding': 'utf-8'}\n",
        "# If that doesn't work, some Windows users might need to uncomment the next line instead\n",
        "# text_loader_kwargs={'autodetect_encoding': True}\n",
        "\n",
        "documents = []\n",
        "for folder in folders:\n",
        "    doc_type = os.path.basename(folder)\n",
        "    loader = DirectoryLoader(folder, glob=\"**/*.md\", loader_cls=TextLoader, loader_kwargs=text_loader_kwargs)\n",
        "    folder_docs = loader.load()\n",
        "    for doc in folder_docs:\n",
        "        doc.metadata[\"doc_type\"] = doc_type\n",
        "        documents.append(doc)"
      ],
      "metadata": {
        "id": "R9P85lThjWNy"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "chunks = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y54bXqXvjoTk",
        "outputId": "3ef2680e-5e44-404c-f9c7-8673d7c6677c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1088, which is longer than the specified 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tr1XXZjXjp6s",
        "outputId": "fce13a8d-d213-448f-cce9-5eddf04551bc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "123"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc_types = set(chunk.metadata['doc_type'] for chunk in chunks)\n",
        "print(f\"Document types found: {', '.join(doc_types)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQdcnBl4jrkp",
        "outputId": "3d9fce3b-6935-44ef-f186-5c7eeec67083"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document types found: knowledge-base\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A sidenote on Embeddings, and \"Auto-Encoding LLMs\"\n",
        "\n",
        "We will be mapping each chunk of text into a Vector that represents the meaning of the text, known as an embedding.\n",
        "\n",
        "OpenAI offers a model to do this, which we will use by calling their API with some LangChain code.\n",
        "\n",
        "This model is an example of an \"Auto-Encoding LLM\" which generates an output given a complete input.\n",
        "It's different to all the other LLMs we've discussed today, which are known as \"Auto-Regressive LLMs\", and generate future tokens based only on past context.\n",
        "\n",
        "Another example of an Auto-Encoding LLMs is BERT from Google. In addition to embedding, Auto-encoding LLMs are often used for classification.\n",
        "\n",
        "### Sidenote\n",
        "\n",
        "In week 8 we will return to RAG and vector embeddings, and we will use an open-source vector encoder so that the data never leaves our computer - that's an important consideration when building enterprise systems and the data needs to remain internal."
      ],
      "metadata": {
        "id": "kcCbRUTyj2Ee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Put the chunks of data into a Vector Store that associates a Vector Embedding with each chunk\n",
        "# Chroma is a popular open source Vector Database based on SQLLite\n",
        "\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=api_key)\n",
        "\n",
        "# If you would rather use the free Vector Embeddings from HuggingFace sentence-transformers\n",
        "# Then replace embeddings = OpenAIEmbeddings()\n",
        "# with:\n",
        "# from langchain.embeddings import HuggingFaceEmbeddings\n",
        "# embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# Delete if already exists\n",
        "\n",
        "if os.path.exists(db_name):\n",
        "    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()\n",
        "\n",
        "# Create vectorstore\n",
        "\n",
        "vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)\n",
        "print(f\"Vectorstore created with {vectorstore._collection.count()} documents\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZoIyHfzj06S",
        "outputId": "8629e6ee-764a-48cf-d3ce-554cc9b6c7c2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vectorstore created with 123 documents\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get one vector and find how many dimensions it has\n",
        "\n",
        "collection = vectorstore._collection\n",
        "sample_embedding = collection.get(limit=1, include=[\"embeddings\"])[\"embeddings\"][0]\n",
        "dimensions = len(sample_embedding)\n",
        "print(f\"The vectors have {dimensions:,} dimensions\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9ingXADj5xY",
        "outputId": "3cccc6da-71da-4c80-94ca-b04fd8a5e1a2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The vectors have 1,536 dimensions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Time to use LangChain to bring it all together"
      ],
      "metadata": {
        "id": "iUQEe9EJkLVJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<table style=\"margin: 0; text-align: left;\">\n",
        "    <tr>\n",
        "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
        "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
        "        </td>\n",
        "        <td>\n",
        "            <h2 style=\"color:#900;\">PLEASE READ ME! Ignoring the Deprecation Warning</h2>\n",
        "            <span style=\"color:#900;\">When you run the next cell, you will get a LangChainDeprecationWarning\n",
        "            about the simple way we use LangChain memory. They ask us to migrate to their new approach for memory.\n",
        "            I feel quite conflicted about this. The new approach involves moving to LangGraph and getting deep into their ecosystem.\n",
        "            There's a fair amount of learning and coding in LangGraph, frankly without much benefit in our case.<br/><br/>\n",
        "            I'm going to think about whether/how to incorporate it in the course, but for now please ignore the Depreciation Warning and\n",
        "            use the code as is; LangChain are not expected to remove ConversationBufferMemory any time soon.\n",
        "            </span>\n",
        "        </td>\n",
        "    </tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "hpvEM_2ukOJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a new Chat with OpenAI\n",
        "llm = ChatOpenAI(temperature=0.7, model_name=MODEL,  openai_api_key=api_key)\n",
        "\n",
        "# set up the conversation memory for the chat\n",
        "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
        "\n",
        "# the retriever is an abstraction over the VectorStore that will be used during RAG. Needed by langchain\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "# putting it together: set up the conversation chain with the GPT 4o-mini LLM, the vector store and memory\n",
        "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhZ97rbokIPP",
        "outputId": "f4d3d991-c4f8-4f05-a338-04db61f7f6a8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-941b6e2a379d>:5: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Can you describe Insurellm in a few sentences\"\n",
        "result = conversation_chain.invoke({\"question\":query}) #Question as a key and then we have to put the message query\n",
        "print(result[\"answer\"]) #Find relevant chunks and sent it to GPT mini\n",
        "\n",
        "query = \"Who is Alex Chen\"\n",
        "result = conversation_chain.invoke({\"question\":query}) #Question as a key and then we have to put the message query\n",
        "print(result[\"answer\"]) #Find relevant chunks and sent it to GPT mini"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACYbXnJBkQtq",
        "outputId": "2b9f9c64-ee44-4fa9-e0c8-8de2facf95de"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Insurellm is an innovative insurance tech startup founded by Avery Lancaster in 2015, aimed at disrupting the insurance industry with its unique products. The company offers four main software products: Carllm for auto insurance, Homellm for home insurance, Rellm for the reinsurance sector, and Marketllm, a marketplace connecting consumers with insurance providers. With 200 employees and over 300 clients worldwide, Insurellm has rapidly expanded, establishing 12 offices across the US by 2024.\n",
            "Alex Chen is a Backend Software Engineer at Insurellm, located in San Francisco, California. He was born on March 15, 1990, and has been with the company since April 2020, starting as a Junior Backend Developer. He was promoted to Backend Software Engineer in October 2021 and awarded the title of Senior Backend Software Engineer in March 2023 for his exemplary performance in scaling backend services. Alex is recognized for his contributions to innovative backend solutions and has also been involved in the Diversity and Inclusion committee at Insurellm.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set up a new conversation memory for the chat\n",
        "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
        "\n",
        "# putting it together: set up the conversation chain with the GPT 4o-mini LLM, the vector store and memory\n",
        "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
      ],
      "metadata": {
        "id": "O_-bgO5RkhjT"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now we will bring this up in Gradio using the Chat interface -\n",
        "\n",
        "A quick and easy way to prototype a chat with an LLM"
      ],
      "metadata": {
        "id": "Ka6K0ydKknDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrapping in a function - note that history isn't used, as the memory is in the conversation_chain\n",
        "\n",
        "def chat(message, history):\n",
        "    result = conversation_chain.invoke({\"question\": message})\n",
        "    return result[\"answer\"]"
      ],
      "metadata": {
        "id": "DJW69ZQEkkB6"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# And in Gradio:\n",
        "\n",
        "view = gr.ChatInterface(chat, type=\"messages\").launch(inbrowser=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        },
        "id": "W-ukst9nklhX",
        "outputId": "ae39a9d2-31a1-45de-84c2-90a5c18db0c5"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://cdc1b1c66bc8d4abf1.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://cdc1b1c66bc8d4abf1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hsQEMEj-kuTj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}